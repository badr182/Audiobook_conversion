{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.  Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Extract the data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_data = np.loadtxt('data.csv',delimiter=',')\n",
    "# excludng the ID and the targets from dataset \n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1] \n",
    "target_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.  Balancing the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (numbre of line) : 14084\n",
      "the number of targets that has One :  2237\n",
      "the number of targets that has Zero : 11847\n",
      " AFTER BALANCING : \n",
      "the number of targets that has One : 2237\n",
      "the number of targets that has One : 2237\n"
     ]
    }
   ],
   "source": [
    "num_one_targets = int(np.sum(target_all))\n",
    "\n",
    "print(\"Shape (numbre of line) :\", target_all.shape[0] )\n",
    "print(\"the number of targets that has One : \", num_one_targets)\n",
    "print(\"the number of targets that has Zero :\", target_all.shape[0] - num_one_targets)\n",
    "\n",
    "zero_targets_counter = 0\n",
    "indices_to_remove = []\n",
    "\n",
    "for i in range( target_all.shape[0] ):\n",
    "    if target_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter >  num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "        \n",
    "print(\" AFTER BALANCING : \")            \n",
    "\n",
    "\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis = 0)\n",
    "targets_equal_priors = np.delete(target_all, indices_to_remove, axis=0)\n",
    "\n",
    "print(\"the number of targets that has One :\", int(sum(targets_equal_priors)))\n",
    "print(\"the number of targets that has One :\", targets_equal_priors.shape[0] - int(sum(targets_equal_priors)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Standardize the inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Shuffle the data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " before shuffle : [   0    1    2 ... 4471 4472 4473]\n",
      " after shuffle  : [1073  334 4314 ... 3835 1533 1223]\n"
     ]
    }
   ],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "\n",
    "print(\" before shuffle :\", shuffled_indices)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "print(\" after shuffle  :\" , shuffled_indices)\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.  Split the dataset into train, validation and test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4474\n",
      "1777.0 3579 0.4965074043028779\n",
      "211.0 447 0.4720357941834452\n",
      "249.0 448 0.5558035714285714\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "print(samples_count)\n",
    "\n",
    "train_sample_count = int(0.8*samples_count)\n",
    "validation_sample_count = int(0.1*samples_count)\n",
    "test_sample_count = samples_count - train_sample_count -validation_sample_count\n",
    "\n",
    "# let's extract them from the dataset \n",
    "\n",
    "# train \n",
    "train_inputs = shuffled_inputs[:train_sample_count]\n",
    "train_targets = shuffled_targets[:train_sample_count]\n",
    "\n",
    "# validation\n",
    "validation_inputs = shuffled_inputs[ train_sample_count:train_sample_count + validation_sample_count ]\n",
    "validation_targets = shuffled_targets[ train_sample_count:train_sample_count + validation_sample_count ]\n",
    "\n",
    "# test \n",
    "test_inputs = shuffled_inputs[ train_sample_count+validation_sample_count:]\n",
    "test_targets = shuffled_targets[ train_sample_count+validation_sample_count:]\n",
    "\n",
    "\n",
    "print(np.sum(train_targets),train_sample_count,np.sum(train_targets)/train_sample_count)\n",
    "print(np.sum(validation_targets),validation_sample_count,np.sum(validation_targets)/validation_sample_count)\n",
    "print(np.sum(test_targets),test_sample_count,np.sum(test_targets)/test_sample_count)\n",
    "# All three sets are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7.  Save the three datasets *.npz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('data_train', input= train_inputs, target=train_targets)\n",
    "np.savez('data_validation', input= validation_inputs, target=validation_targets)\n",
    "np.savez('data_test', input= test_inputs, target=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. Create a class that will do the batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader():\n",
    "    \n",
    "   \n",
    "    def __init__(self,dataset,batch_size = None):\n",
    "\n",
    "        npz = np.load('data_{0}.npz'.format(dataset) )\n",
    "        self.inputs, self.targets = npz['input'].astype(np.float), npz['target'].astype(np.int)\n",
    "        \n",
    "        if batch_size is None:\n",
    "            self.batch_size = self.inputs.shape[0]\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.curr_batch = 0\n",
    "        self.batch_count = self.inputs.shape[0] // self.batch_size\n",
    "    \n",
    "    \n",
    "    \n",
    "    # A method which loads the next batch \n",
    "    # the next function slices the next batch out of the dataset and load it \n",
    "    def __next__(self):\n",
    "        \n",
    "        if self.curr_batch >= self.batch_count:\n",
    "            self.curr_batch = 0\n",
    "            raise StopIteration()\n",
    "        \n",
    "        # You slice the dataset in batches and then the \"next\" function loads them one after other \n",
    "        batch = slice(self.curr_batch * self.batch_size ,(self.curr_batch + 1) * self.batch_size)\n",
    "        inputs_batch = self.inputs[batch] \n",
    "        targets_batch = self.targets[batch]\n",
    "        self.curr_batch += 1\n",
    "  \n",
    "        \n",
    "        # the function will return the inputs batch and the targets batch \n",
    "        # return inputs_batch, targets_one_hot\n",
    "        return inputs_batch, targets_batch\n",
    "    \n",
    "    # A method needed for iterating over the batches, as we will put them in a loop\n",
    "    # This tells python that the class we're definig is iterable, i.e that we can  use it \n",
    "    def __iter__(self):\n",
    "        return self \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9. Create the machine learning algorithm   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "output_size = 2 # such we have 2 input \n",
    "hidden_layer_size = 50 # hyperparameter\n",
    "\n",
    "# To clear the defined variables and operations of the previous cell\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  A placeholder is simply a variable that we will assign data to at a later date. \n",
    "#  It allows us to create our operations and build our computation graph, \n",
    "#  without needing the data. In TensorFlow terminology, \n",
    "#  we then feed data into the graph through these placeholders.\n",
    "#  place in memory where we will store value later on\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.int32, [None, output_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Hiden layer 1 ###\n",
    "weights_1 = tf.get_variable(\"weights_1\",[input_size,hidden_layer_size])\n",
    "biases_1 = tf.get_variable(\"biases_1\",[hidden_layer_size])\n",
    "# time to applay an activation function\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs,weights_1) + biases_1 )\n",
    "\n",
    "\n",
    "### Hiden layer 2 ###\n",
    "weights_1 = tf.get_variable(\"weights_2\",[hidden_layer_size,hidden_layer_size])\n",
    "biases_2 = tf.get_variable(\"biases_2\",[hidden_layer_size])\n",
    "# time to applay an activation function\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1,weights_1) + biases_2)\n",
    "\n",
    "\n",
    "### Output layers ###\n",
    "weights_3 = tf.get_variable(\"weights_3\",[hidden_layer_size,output_size])\n",
    "biases_3 = tf.get_variable(\"biases_3\",[output_size])\n",
    "outputs = tf.matmul(outputs_2,weights_3) + biases_3\n",
    "\n",
    "# That's how we stauck layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-415ed51f1274>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# is a function that apply a softmax activation and calculates a cross entropy loss\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits = outputs, labels = targets) \n",
    "# tf.reduce_mean() is a method which finds the mean of the elements of a tensor across a dimension\n",
    "mean_loss = tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have the model and the loss\n",
    "# choose the optimization method\n",
    "optimize = tf.train.AdamOptimizer( learning_rate=0.001 ).minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.equal() is a method checks if two values are equal. in the case of tensors so element-wise\n",
    "# however to show we only care about the column indices we include a second argument to the Tf.argmax\n",
    "# compare the argmax of outputs and argmax of targets\n",
    "outputs_equals_target = tf.equal( tf.argmax(outputs,1),tf.argmax(targets,1) ) \n",
    "\n",
    "\n",
    "# example of horse cat dogin this case out_equals_ target = [1,0,1,1] T\n",
    "# the accuracy is the mean of the out_equals_ target vector mean = (1+0+1+1)/4 = 0,75\n",
    "# already this done throuth the tf reduce mean \n",
    "accuracy = tf.reduce_mean( tf.cast(outputs_equals_target, tf.float32) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (100,) for Tensor 'Placeholder_1:0', which has shape '(?, 2)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f674ae9b0403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Since the class is iterable, we can iterate over data using the code :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtarget_batch\u001b[0m \u001b[0;34m}\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mcurr_epoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Numbers of batches: train_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1149\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1150\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (100,) for Tensor 'Placeholder_1:0', which has shape '(?, 2)'"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "\n",
    "# we want a number small enough to learn faster but big enough to preserve the underlying dependencies\n",
    "batch_size = 100 \n",
    "\n",
    "# i'll change the max number of epochs to 50 as i already know how the algo will behave  \n",
    "max_epochs = 50\n",
    "# the true early stopping will come if the validation loss starts increasing\n",
    "# this value large enough to ensure the early stpping won't be triggered on the first epoch\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "# we preprocessed the data on our. Now we must load it \n",
    "# both variables will be instance of the Audiobooks_Data_Reader class\n",
    "train_data = Reader('train', batch_size)\n",
    "validation_data = Reader('validation')\n",
    "\n",
    "for epoch_counter in range(max_epochs):\n",
    "    ### Training  ###\n",
    "    curr_epoch_loss = 0.\n",
    "    # Since the class is iterable, we can iterate over data using the code : \n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_loss = sess.run([optimize,mean_loss], feed_dict ={inputs: input_batch, targets:target_batch } )\n",
    "        curr_epoch_loss += batch_loss\n",
    "    # Numbers of batches: train_data    \n",
    "    curr_epoch_loss /= train_data.batch_count \n",
    "    ### end Training  ###\n",
    "    \n",
    "    # time to validate \n",
    "    # notice : \n",
    "    # Audiobooks_(\"train\", 5) take batches of 5 samples at a time \n",
    "    # Audiobooks_(\"validation\") take the whole data in a single batch\n",
    "    validation_loss = 0.\n",
    "    validation_accuracy = 0.\n",
    "    # validation \n",
    "    for input_batch, target_batch in validation_data:\n",
    "        validation_loss, validation_accuracy = sess.run([mean_loss,accuracy], feed_dict ={inputs: input_batch, targets:target_batch } )\n",
    "    \n",
    "    # so we will simply forward propagate the whole validation data set through the net \n",
    "    \n",
    "    # Forward propagation does not require a loop ( we simply feed the model with data )\n",
    "    # However, our class was an iterator (to be used with loops )\n",
    "    # Logically, this loop will always have a single iteration \n",
    "    # using a loop won't affect the speed of the algo \n",
    "    \n",
    "    print('Epoch '+  str(epoch_counter+1) +\n",
    "      '. Trining loss:'+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "      '. Validation loss:'+'{0:.3f}'.format(validation_loss)+\n",
    "      '. Validation accuracy:'+'{0:.3f}'.format(validation_accuracy * 100.)+'%'\n",
    "     )\n",
    "    \n",
    "    # breaks the loop when the validation loss i higher than the previous validation \n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break \n",
    "    \n",
    "    prev_validation_loss = validation_loss\n",
    "    \n",
    "    # In this way we wil know when the valdation loss starts increasing \n",
    "\n",
    "print(\"End of training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
